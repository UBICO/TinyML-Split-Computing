{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ModelDataGenerator import ModelDataGenerator\n",
    "from ModelBuilder import ModelBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of ModelDataGenerator\n",
    "generator = ModelDataGenerator(\n",
    "    image_size=30, \n",
    "    num_images=2000,\n",
    "    dataset_path='models/test_model/data'\n",
    ")\n",
    "\n",
    "# Generate and save the dataset\n",
    "image_paths, labels = generator.generate_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "50/50 [==============================] - 1s 18ms/step - loss: 0.6463 - accuracy: 0.8300 - val_loss: 0.4517 - val_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "50/50 [==============================] - 1s 14ms/step - loss: 0.1350 - accuracy: 1.0000 - val_loss: 0.0070 - val_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "50/50 [==============================] - 1s 14ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 9.0642e-04 - val_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 5.7941e-04 - accuracy: 1.0000 - val_loss: 3.5859e-04 - val_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "50/50 [==============================] - 1s 14ms/step - loss: 2.6726e-04 - accuracy: 1.0000 - val_loss: 1.9605e-04 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "50/50 [==============================] - 1s 14ms/step - loss: 1.5664e-04 - accuracy: 1.0000 - val_loss: 1.2444e-04 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "50/50 [==============================] - 1s 15ms/step - loss: 1.0396e-04 - accuracy: 1.0000 - val_loss: 8.5975e-05 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "50/50 [==============================] - 1s 14ms/step - loss: 7.4107e-05 - accuracy: 1.0000 - val_loss: 6.3496e-05 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 5.5782e-05 - accuracy: 1.0000 - val_loss: 4.8672e-05 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "50/50 [==============================] - 1s 14ms/step - loss: 4.3529e-05 - accuracy: 1.0000 - val_loss: 3.8544e-05 - val_accuracy: 1.0000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3.8544e-05 - accuracy: 1.0000\n",
      "Test Loss: 0.0000, Test Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Fabio Bove\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model = ModelBuilder(\n",
    "    image_size=30, \n",
    "    num_classes=1, \n",
    "    image_paths=image_paths, \n",
    "    labels= labels, \n",
    "    dataset_path='models/test_model/data'\n",
    ")\n",
    "model.train_model(epochs=10)\n",
    "model.evaluate_model()\n",
    "model.save_model_as_h5(model_name='test_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 62ms/step\n",
      "Model predicts a 1\n",
      "values is a 1\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Model does not predict a 1\n",
      "values is a 0\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Model does not predict a 1\n",
      "values is a 0\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "Model does not predict a 1\n",
      "values is a 0\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Model predicts a 1\n",
      "values is a 1\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Model predicts a 1\n",
      "values is a 1\n"
     ]
    }
   ],
   "source": [
    "# Get the prediction from the model\n",
    "data_for_predictions  = [\n",
    "    \"./models/test_model/pred_data/pred_test_is_1.png\",\n",
    "    \"./models/test_model/pred_data/pred_test_is_0.png\",\n",
    "    \"./models/test_model/pred_data/pred_test_is_0.png\",\n",
    "    \"./models/test_model/pred_data/pred_test_is_0.png\",\n",
    "    \"./models/test_model/pred_data/pred_test_is_1.png\",\n",
    "    \"./models/test_model/pred_data/pred_test_is_1.png\",\n",
    "]\n",
    "\n",
    "# Set a threshold for classification (you can adjust this based on your needs)\n",
    "threshold = 0.5\n",
    "\n",
    "for data in data_for_predictions:\n",
    "    \n",
    "    prediction = model.predict(data)\n",
    "    # Check if the prediction is greater than the threshold\n",
    "    if prediction[0][0] >= threshold:\n",
    "        print(\"Model predicts a 1\")\n",
    "    else:\n",
    "        print(\"Model does not predict a 1\")\n",
    "    correct_value = data.split('_')[-1].split('.')[0]\n",
    "    print(f\"values is a {correct_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KerasTensor(type_spec=TensorSpec(shape=(None, 30, 30, 3), dtype=tf.float32, name='layer_0_input'), name='layer_0_input', description=\"created by layer 'layer_0_input'\") KerasTensor(type_spec=TensorSpec(shape=(None, 28, 28, 32), dtype=tf.float32, name=None), name='layer_0/Relu:0', description=\"created by layer 'layer_0'\")\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "\n",
      "layer_0, 802816, 93.838623046875\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 28, 28, 32), dtype=tf.float32, name=None), name='layer_0/Relu:0', description=\"created by layer 'layer_0'\") KerasTensor(type_spec=TensorSpec(shape=(None, 14, 14, 32), dtype=tf.float32, name=None), name='layer_1/MaxPool:0', description=\"created by layer 'layer_1'\")\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "\n",
      "layer_1, 200704, 98.216552734375\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 14, 14, 32), dtype=tf.float32, name=None), name='layer_1/MaxPool:0', description=\"created by layer 'layer_1'\") KerasTensor(type_spec=TensorSpec(shape=(None, 6272), dtype=tf.float32, name=None), name='layer_2/Reshape:0', description=\"created by layer 'layer_2'\")\n",
      "1/1 [==============================] - 0s 291ms/step\n",
      "\n",
      "layer_2, 200704, 338.076416015625\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 6272), dtype=tf.float32, name=None), name='layer_2/Reshape:0', description=\"created by layer 'layer_2'\") KerasTensor(type_spec=TensorSpec(shape=(None, 64), dtype=tf.float32, name=None), name='layer_3/Relu:0', description=\"created by layer 'layer_3'\")\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "\n",
      "layer_3, 2048, 96.91943359375\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 64), dtype=tf.float32, name=None), name='layer_3/Relu:0', description=\"created by layer 'layer_3'\") KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='layer_4/Sigmoid:0', description=\"created by layer 'layer_4'\")\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "\n",
      "layer_4, 32, 94.217041015625\n",
      "Model does not predict a 1\n",
      "values is a 0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "import time\n",
    "import csv \n",
    "\n",
    "\n",
    "def calculate_size_in_bits(layer_output):\n",
    "    # Get the data type of the layer output\n",
    "    dtype = layer_output.dtype\n",
    "    # Calculate the size in bits\n",
    "    size_in_bits = tf.reduce_prod(layer_output.shape) * tf.constant(dtype.itemsize * 8)\n",
    "    return size_in_bits.numpy()  # Convert to a Python scalar for writing to CSV\n",
    "\n",
    "def create_analytics_csv(analytics_path, layer_name, layer_size, layer_inference_time):\n",
    "    with open(analytics_path, mode='a', newline='') as csv_file:\n",
    "        fieldnames = ['layer', 'layer_size', 'layer_inference_time']\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "\n",
    "        # Check if the file is empty and write the header if needed\n",
    "        if csv_file.tell() == 0:\n",
    "            writer.writeheader()\n",
    "\n",
    "        writer.writerow({\n",
    "            'layer': layer_name,\n",
    "            'layer_size': layer_size,\n",
    "            'layer_inference_time': layer_inference_time\n",
    "        })\n",
    "    print(f\"\\n{layer_name}, {layer_size}, {layer_inference_time}\")\n",
    "\n",
    "def load_model(model_path='models/test_model/test_model.h5'):\n",
    "    return tf.keras.models.load_model(model_path)\n",
    "\n",
    "def predict_single_layer(model, layer_id, layer_input_data, analytics_path):\n",
    "    layer = model.layers[layer_id]\n",
    "    # Create an intermediate model with the current layer\n",
    "    intermediate_model = tf.keras.Model(inputs=layer.input, outputs=layer.output)\n",
    "    print(layer.input, layer.output)\n",
    "    # Predict using the current layer, keeps track of the time it takes\n",
    "    t_begin = time.time() * 1000\n",
    "    layer_output = intermediate_model.predict(layer_input_data)\n",
    "    t_end = time.time() * 1000\n",
    "\n",
    "    create_analytics_csv(\n",
    "        analytics_path = analytics_path,\n",
    "        layer_name = layer.name,\n",
    "        layer_inference_time = t_end - t_begin,\n",
    "        layer_size = calculate_size_in_bits(layer_output)\n",
    "    )\n",
    "    \n",
    "    return layer_output\n",
    "\n",
    "def perform_predict(model, start_layer_index=0, input_data='', analytics_path=''):\n",
    "    num_layers = len(model.layers)\n",
    "    predictions = []\n",
    "\n",
    "    # Load and preprocess the input image for the first layer\n",
    "    input_image = load_img(input_data, target_size=(30, 30))\n",
    "    input_array = img_to_array(input_image)\n",
    "    input_array = tf.expand_dims(input_array, 0)  # Create batch axis\n",
    "    input_array = input_array / 255.0  # Normalize pixel values to be between 0 and 1\n",
    "\n",
    "    for layer_id in range(start_layer_index, num_layers):\n",
    "        # For the first layer, use the preprocessed input image\n",
    "        if layer_id == 0:\n",
    "            # Resize the input to match the expected shape of the first layer\n",
    "            input_array = tf.image.resize(input_array, (30, 30))\n",
    "            layer_output = predict_single_layer(model, layer_id, input_array, analytics_path)\n",
    "        else:\n",
    "            # For subsequent layers, reshape the output of the previous layer if needed\n",
    "            previous_layer_output = predictions[-1]\n",
    "            expected_input_shape = model.layers[layer_id].input_shape[1:]  # Exclude batch dimension\n",
    "            if previous_layer_output.shape[1:] != expected_input_shape:\n",
    "                previous_layer_output = tf.image.resize(previous_layer_output, expected_input_shape)\n",
    "            layer_output = predict_single_layer(model, layer_id, previous_layer_output, analytics_path)\n",
    "\n",
    "        predictions.append(layer_output)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "model = load_model(model_path='./models/test_model/test_model.h5')\n",
    "input_data = './models/test_model/pred_data/pred_test_is_0.png'\n",
    "analytics_path = './models/test_model/analytics_data/analytics.csv'\n",
    "predictions = perform_predict(model=model, input_data=input_data, analytics_path=analytics_path)\n",
    "prediction = predictions[-1][0]\n",
    "threshold= 0.5\n",
    "# Check if the prediction is greater than the threshold\n",
    "if prediction >= threshold:\n",
    "    print(\"Model predicts a 1\")\n",
    "else:\n",
    "    print(\"Model does not predict a 1\")\n",
    "correct_value = input_data.split('_')[-1].split('.')[0]\n",
    "print(f\"values is a {correct_value}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
